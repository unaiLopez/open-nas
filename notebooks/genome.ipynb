{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fa5325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import pprint\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "# ─── 1. CONFIGURACIÓN ─────────────────────────────────────────────────────────\n",
    "\n",
    "# Semilla para reproducibilidad\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Opciones de hiperparámetros (puedes expandir esto)\n",
    "ACTIVATIONS = ['relu', 'leaky_relu', 'gelu']\n",
    "POOLING_TYPES = [None, 'max', 'avg']\n",
    "DROPOUT_PROB = [0.0, 0.1, 0.25]\n",
    "CONV_OUT_CHANNELS = [16, 32, 64, 128]\n",
    "CONV_KERNEL_SIZES = [3, 5]\n",
    "CONV_STRIDES = [1, 2]\n",
    "POOLING_KERNEL_SIZES = [2, 3]\n",
    "POOLING_STRIDES = [2, 3]\n",
    "LINEAR_UNITS = [128, 256, 512]\n",
    "\n",
    "# ─── 2. GENOMA BASADO EN GRAFOS (NUEVA ESTRUCTURA) ───────────────────────────\n",
    "\n",
    "def create_empty_genome() -> Dict[str, Any]:\n",
    "    \"\"\"Inicializa un genoma con su estructura de grafo: nodos y aristas.\"\"\"\n",
    "    return {\"layers\": [], \"connections\": []}\n",
    "\n",
    "def add_layer_to_genome(genome: Dict[str, Any], layer_gene: Dict[str, Any]) -> int:\n",
    "    \"\"\"Añade un nuevo gen de capa al genoma, asignándole un ID único.\"\"\"\n",
    "    new_id = len(genome['layers'])\n",
    "    layer_gene['id'] = new_id\n",
    "    genome['layers'].append(layer_gene)\n",
    "    return new_id\n",
    "\n",
    "def add_connection_to_genome(genome: Dict[str, Any], from_id: int, to_id: int) -> None:\n",
    "    \"\"\"Añade una conexión entre dos capas.\"\"\"\n",
    "    connection = (from_id, to_id)\n",
    "    if connection not in genome['connections']:\n",
    "        genome['connections'].append(connection)\n",
    "\n",
    "def get_always_valid_kernels(height: int, width: int) -> List[int]:\n",
    "    # Valid kernel sizes: must be ≤ spatial dims\n",
    "    max_kernel = min(height, width, max(CONV_KERNEL_SIZES))\n",
    "    valid_kernels = [k for k in CONV_KERNEL_SIZES if k <= max_kernel]\n",
    "    if not valid_kernels:\n",
    "        valid_kernels = [1]\n",
    "    return valid_kernels\n",
    "\n",
    "def get_always_valid_strides(height: int, width: int) -> List[int]:\n",
    "    valid_strides = [\n",
    "        s for s in CONV_STRIDES\n",
    "        if (height // s >= 1) and (width // s >= 1)\n",
    "    ]\n",
    "    if not valid_strides:\n",
    "        valid_strides = [1]\n",
    "    return valid_strides\n",
    "\n",
    "def get_valid_pooling_kernels_or_return_empty_list(height: int, width: int) -> List[int]:\n",
    "    min_dim = min(height, width)\n",
    "    valid_kernels = [k for k in POOLING_KERNEL_SIZES if (min_dim - k) // k + 1 <= min_dim] # assuming kernel_size = strides and dilation = 1\n",
    "    return valid_kernels\n",
    "\n",
    "def get_conv_block_gene(input_shape: Tuple[int, int, int]) -> Dict[str, Any]:\n",
    "    in_ch, h, w = input_shape\n",
    "\n",
    "    valid_kernels = get_always_valid_kernels(h, w)\n",
    "    valid_strides = get_always_valid_strides(h, w)\n",
    "    kernel_size = random.choice(valid_kernels)\n",
    "    stride = random.choice(valid_strides)\n",
    "    padding = (kernel_size - 1) // 2  # Same padding\n",
    "\n",
    "    out_channels = random.choice(CONV_OUT_CHANNELS)\n",
    "\n",
    "    # Compute output shape after conv\n",
    "    h_out = (h + 2 * padding - kernel_size) // stride + 1\n",
    "    w_out = (w + 2 * padding - kernel_size) // stride + 1\n",
    "    \n",
    "    # Pooling decision\n",
    "    pool_type = random.choice(POOLING_TYPES)\n",
    "    pooling_kernel = None\n",
    "    if pool_type:\n",
    "        valid_pooling_kernels = get_valid_pooling_kernels_or_return_empty_list(h_out, w_out)\n",
    "        if valid_pooling_kernels:\n",
    "            pooling_kernel = random.choice(valid_pooling_kernels)\n",
    "            # Compute output shape after pooling\n",
    "            # We assume kernel_size = strides and dilation = 1\n",
    "            h_out = (h_out - pooling_kernel) // pooling_kernel + 1\n",
    "            w_out = (w_out - pooling_kernel) // pooling_kernel + 1\n",
    "        else:\n",
    "            pool_type = None\n",
    "\n",
    "    return {\n",
    "        'type': 'conv',\n",
    "        'in_channels': in_ch,\n",
    "        'out_channels': out_channels,\n",
    "        'kernel': kernel_size,\n",
    "        'stride': stride,\n",
    "        'padding': padding,\n",
    "        'activation': random.choice(ACTIVATIONS),\n",
    "        'use_bn': random.choice([True, False]),\n",
    "        'pool': {'type': pool_type, 'kernel': pooling_kernel} if pool_type else None,\n",
    "        'output_shape': (out_channels, h_out, w_out)\n",
    "    }\n",
    "\n",
    "def get_linear_block_gene(in_features: int) -> Dict[str, Any]:\n",
    "    out_features = random.choice(LINEAR_UNITS)\n",
    "    return {\n",
    "        'type': 'linear',\n",
    "        'in_features': in_features,\n",
    "        'out_features': out_features,\n",
    "        'activation': random.choice(ACTIVATIONS),\n",
    "        'dropout': random.choice(DROPOUT_PROB),\n",
    "        'output_shape': (out_features,)\n",
    "    }\n",
    "\n",
    "# ─── 3. GENERACIÓN ROBUSTA DE GENOMAS ──────────────────────────────────────────\n",
    "\n",
    "def random_genome_graph(\n",
    "    input_shape=(3, 32, 32),\n",
    "    min_conv_layers=2, max_conv_layers=5,\n",
    "    min_linear_layers=1, max_linear_layers=2,\n",
    "    num_classes=10,\n",
    "    skip_connection_prob=0.3\n",
    "):\n",
    "    \"\"\"\n",
    "    Construye un genoma de red neuronal como un grafo (DAG).\n",
    "\n",
    "    Esta función es robusta porque:\n",
    "      - Rastrea las formas de los tensores para garantizar la validez de las capas.\n",
    "      - Construye un grafo acíclico dirigido (DAG) por diseño.\n",
    "      - Puede crear múltiples ramas y unirlas con conexiones de salto.\n",
    "    \"\"\"\n",
    "    genome = create_empty_genome()\n",
    "    shape_tracker = {} # Diccionario para rastrear la forma de salida de cada capa\n",
    "\n",
    "    # --- Capa de Entrada ---\n",
    "    in_channels, h, w = input_shape\n",
    "    input_id = add_layer_to_genome(genome, {'type': 'input'})\n",
    "    shape_tracker[input_id] = (in_channels, h, w)\n",
    "    \n",
    "    last_conv_layer_id = input_id\n",
    "    conv_layer_ids = []\n",
    "\n",
    "    # --- Bloques Convolucionales ---\n",
    "    num_conv_layers = random.randint(min_conv_layers, max_conv_layers)\n",
    "    for _ in range(num_conv_layers):\n",
    "        gene = get_conv_block_gene(\n",
    "            shape_tracker[last_conv_layer_id]\n",
    "        )\n",
    "        new_id = add_layer_to_genome(genome, gene)\n",
    "        add_connection_to_genome(genome, last_conv_layer_id, new_id)\n",
    "        shape_tracker[new_id] = gene['output_shape']\n",
    "\n",
    "        # Skip connections\n",
    "        if len(conv_layer_ids) > 0 and random.random() < skip_connection_prob:\n",
    "            # There are no constraints because the shapes will be matched under demand with pytorch on forward pass\n",
    "            skip_source_id = random.choice(conv_layer_ids)\n",
    "            add_connection_to_genome(genome, skip_source_id, new_id)\n",
    "\n",
    "        last_conv_layer_id = new_id\n",
    "        conv_layer_ids.append(new_id)\n",
    "\n",
    "    # --- Transición a Capas Lineales (Flatten o Pooling Global) ---\n",
    "    last_feature_id = last_conv_layer_id\n",
    "    in_ch, h, w = shape_tracker[last_feature_id]\n",
    "    \n",
    "    if random.choice([True, False]):\n",
    "        gene = {'type': 'global_pool', 'pool_type': 'adaptive_avg'}\n",
    "        in_features = in_ch\n",
    "    else:\n",
    "        gene = {'type': 'flatten'}\n",
    "        in_features = in_ch * h * w\n",
    "\n",
    "    transition_id = add_layer_to_genome(genome, gene)\n",
    "    add_connection_to_genome(genome, last_feature_id, transition_id)\n",
    "    shape_tracker[transition_id] = (in_features,)\n",
    "    last_linear_id = transition_id\n",
    "\n",
    "    # --- Bloques Lineales ---\n",
    "    num_linear_layers = random.randint(min_linear_layers, max_linear_layers)\n",
    "    for _ in range(num_linear_layers):\n",
    "        gene = get_linear_block_gene(in_features)\n",
    "        new_id = add_layer_to_genome(genome, gene)\n",
    "        add_connection_to_genome(genome, last_linear_id, new_id)\n",
    "        \n",
    "        shape_tracker[new_id] = gene[\"output_shape\"]\n",
    "        last_linear_id = new_id\n",
    "\n",
    "    # --- Capa de Salida ---\n",
    "    last_layer_input_features = gene[\"output_shape\"][0]\n",
    "    gene = {\n",
    "        'type': 'linear',\n",
    "        'in_features': last_layer_input_features,\n",
    "        'out_features': num_classes,\n",
    "        'activation': 'none', # La activación final (softmax) se aplica en la función de pérdida\n",
    "        'dropout': 0.0\n",
    "    }\n",
    "    output_id = add_layer_to_genome(genome, gene)\n",
    "    add_connection_to_genome(genome, last_linear_id, output_id)\n",
    "    add_layer_to_genome(genome, {'type': 'output', 'from': output_id}) # Nodo final simbólico\n",
    "    print(shape_tracker)\n",
    "\n",
    "    return genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e40b35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: (3, 32, 32), 1: (128, 16, 16), 2: (128, 2, 2), 3: (128, 0, 0), 4: (32, 0, 0), 5: (16, 0, 0), 6: (16,), 7: (128,), 8: (128,)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 9)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_genome_graph(\n",
    "    input_shape=(3, 32, 32),\n",
    "    min_conv_layers=2, max_conv_layers=5,\n",
    "    min_linear_layers=1, max_linear_layers=2,\n",
    "    num_classes=10,\n",
    "    skip_connection_prob=0.5\n",
    ")[\"connections\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "fe15fb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 4. CONSTRUCTOR DEL MODELO A PARTIR DEL GRAFO ──────────────────────────────\n",
    "\n",
    "class MergeLayer(nn.Module):\n",
    "    \"\"\"Capa para fusionar varias entradas concatenándolas en la dimensión del canal.\"\"\"\n",
    "    def forward(self, *inputs):\n",
    "        # Asegurarse de que todos los tensores de entrada tengan las mismas dimensiones espaciales\n",
    "        # antes de la concatenación. Este chequeo es opcional pero recomendado.\n",
    "        # for i in range(1, len(inputs)):\n",
    "        #     assert inputs[i].shape[2:] == inputs[0].shape[2:], \"Las dimensiones espaciales deben coincidir para la fusión\"\n",
    "        return torch.cat(inputs, dim=1)\n",
    "\n",
    "class GenomeNet(nn.Module):\n",
    "    def __init__(self, genome):\n",
    "        super().__init__()\n",
    "        self.genome = genome\n",
    "        self.torch_layers = nn.ModuleDict()\n",
    "        self._build_model()\n",
    "\n",
    "    def _get_input_ids_for(self, layer_id):\n",
    "        \"\"\"Encuentra todos los IDs de capas que conectan a la capa layer_id.\"\"\"\n",
    "        return [src for src, dest in self.genome['connections'] if dest == layer_id]\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"Construye las capas de PyTorch basándose en los genes del genoma.\"\"\"\n",
    "        # Un mapa para rastrear el número de canales de entrada después de las fusiones\n",
    "        in_channels_map = {}\n",
    "\n",
    "        for gene in self.genome['layers']:\n",
    "            layer_id = gene['id']\n",
    "            layer_type = gene.get('type')\n",
    "            \n",
    "            # Determinar las entradas para esta capa\n",
    "            input_ids = self._get_input_ids_for(layer_id)\n",
    "            \n",
    "            current_in_channels = 0\n",
    "            if input_ids:\n",
    "                # Si hay múltiples entradas, se fusionarán. La entrada total de canales es la suma.\n",
    "                if len(input_ids) > 1:\n",
    "                    self.torch_layers[f\"merge_{layer_id}\"] = MergeLayer()\n",
    "                    current_in_channels = sum(in_channels_map[i][0] for i in input_ids)\n",
    "                else: # Una sola entrada\n",
    "                    current_in_channels = in_channels_map[input_ids[0]][0]\n",
    "\n",
    "            # Instanciar la capa de PyTorch correspondiente\n",
    "            if layer_type == 'conv':\n",
    "                gene['in_channels'] = current_in_channels\n",
    "                block = self._create_conv_block(gene)\n",
    "                self.torch_layers[str(layer_id)] = block\n",
    "                in_channels_map[layer_id] = (gene['out_channels'],)\n",
    "\n",
    "            elif layer_type == 'linear':\n",
    "                block = self._create_linear_block(gene)\n",
    "                self.torch_layers[str(layer_id)] = block\n",
    "                in_channels_map[layer_id] = (gene['out_features'],)\n",
    "                \n",
    "            elif layer_type == 'flatten':\n",
    "                self.torch_layers[str(layer_id)] = nn.Flatten()\n",
    "                # La forma se determina dinámicamente, no es necesario almacenarla aquí.\n",
    "                \n",
    "            elif layer_type == 'global_pool':\n",
    "                pool_size = (1, 1)\n",
    "                if gene['pool_type'] == 'adaptive_avg':\n",
    "                    self.torch_layers[str(layer_id)] = nn.Sequential(nn.AdaptiveAvgPool2d(pool_size), nn.Flatten())\n",
    "                elif gene['pool_type'] == 'adaptive_max':\n",
    "                    self.torch_layers[str(layer_id)] = nn.Sequential(nn.AdaptiveMaxPool2d(pool_size), nn.Flatten())\n",
    "                in_channels_map[layer_id] = (current_in_channels,)\n",
    "            \n",
    "            elif layer_type == 'input':\n",
    "                # El nodo de entrada no tiene capa de PyTorch; solo es un punto de partida.\n",
    "                in_channels_map[layer_id] = (self.genome['layers'][0].get('shape', (3, 32, 32))[0],)\n",
    "\n",
    "    def _create_conv_block(self, gene):\n",
    "        layers = []\n",
    "        layers.append(nn.Conv2d(\n",
    "            in_channels=gene['in_channels'], out_channels=gene['out_channels'],\n",
    "            kernel_size=gene['kernel'], stride=gene['stride'], padding=gene['kernel'] // 2\n",
    "        ))\n",
    "        if gene['use_bn']:\n",
    "            layers.append(nn.BatchNorm2d(gene['out_channels']))\n",
    "        if gene['activation'] == 'relu':\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "        elif gene['activation'] == 'leaky_relu':\n",
    "            layers.append(nn.LeakyReLU(inplace=True))\n",
    "        elif gene['activation'] == 'gelu':\n",
    "            layers.append(nn.GELU())\n",
    "        \n",
    "        pool = gene.get('pool')\n",
    "        if pool:\n",
    "            pool_type = pool.get('type')\n",
    "            if pool_type == 'max':\n",
    "                pool_kernel = gene.get('pool').get('kernel')\n",
    "                layers.append(nn.MaxPool2d(kernel_size=pool_kernel))\n",
    "            elif pool_type == 'avg':\n",
    "                pool_kernel = gene.get('pool').get('kernel')\n",
    "                layers.append(nn.AvgPool2d(kernel_size=pool_kernel))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _create_linear_block(self, gene):\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(gene['in_features'], gene['out_features']))\n",
    "        if gene['activation'] == 'relu':\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "        elif gene['activation'] == 'leaky_relu':\n",
    "            layers.append(nn.LeakyReLU(inplace=True))\n",
    "        elif gene['activation'] == 'gelu':\n",
    "            layers.append(nn.GELU())\n",
    "        if gene['dropout'] > 0:\n",
    "            layers.append(nn.Dropout(p=gene['dropout']))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Diccionario para almacenar las salidas de cada capa\n",
    "        outputs = {}\n",
    "        \n",
    "        # El nodo 0 es la entrada\n",
    "        outputs[0] = x\n",
    "        \n",
    "        # Iterar a través de las capas en orden de ID (garantiza el orden topológico)\n",
    "        for layer_id_str, torch_layer in self.torch_layers.items():\n",
    "            if \"merge\" in layer_id_str:\n",
    "                continue # Los merges se manejan bajo demanda\n",
    "            \n",
    "            layer_id = int(layer_id_str)\n",
    "            input_ids = self._get_input_ids_for(layer_id)\n",
    "            \n",
    "            # Recopilar tensores de entrada\n",
    "            input_tensors = [outputs[i] for i in input_ids]\n",
    "            \n",
    "            # Fusionar si es necesario\n",
    "            if len(input_tensors) > 1:\n",
    "                merge_layer = self.torch_layers[f\"merge_{layer_id}\"]\n",
    "                model_input = merge_layer(*input_tensors)\n",
    "            else:\n",
    "                model_input = input_tensors[0]\n",
    "\n",
    "            # Pasar por la capa actual\n",
    "            outputs[layer_id] = torch_layer(model_input)\n",
    "            \n",
    "        # El último nodo del genoma es el de tipo 'output'\n",
    "        output_node = next(g for g in self.genome['layers'] if g.get('type') == 'output')\n",
    "        final_tensor = outputs[output_node['from']]\n",
    "        return final_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "2175f4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 5. ALGORITMO GENÉTICO: MUTACIÓN Y CRUCE ───────────────────────────────\n",
    "\n",
    "def mutate_genome(genome, mutation_rate=0.1):\n",
    "    \"\"\"\n",
    "    Aplica mutaciones a un genoma. Tipos de mutación:\n",
    "    1. Mutación de hiperparámetros de una capa.\n",
    "    2. Adición de una nueva conexión de salto (skip connection).\n",
    "    \"\"\"\n",
    "    new_genome = copy.deepcopy(genome) # Trabajar sobre una copia\n",
    "    \n",
    "    # 1. Mutar hiperparámetros de capas existentes\n",
    "    for gene in new_genome['layers']:\n",
    "        if random.random() < mutation_rate:\n",
    "            if gene['type'] == 'conv':\n",
    "                gene['kernel'] = random.choice([3, 5])\n",
    "                gene['activation'] = random.choice(ACTIVATIONS)\n",
    "            elif gene['type'] == 'linear' and gene.get('activation') != 'none':\n",
    "                gene['dropout'] = random.choice(DROPOUT_PROB)\n",
    "                gene['activation'] = random.choice(ACTIVATIONS)\n",
    "    \n",
    "    # 2. Añadir una nueva conexión de salto\n",
    "    if random.random() < mutation_rate:\n",
    "        conv_layers = [g for g in new_genome['layers'] if g['type'] == 'conv']\n",
    "        if len(conv_layers) >= 2:\n",
    "            source_gene = random.choice(conv_layers)\n",
    "            dest_gene = random.choice(conv_layers)\n",
    "            \n",
    "            # Asegurar que la conexión sea hacia adelante y no inmediata\n",
    "            if source_gene['id'] < dest_gene['id'] - 1:\n",
    "                # Este es un intento simple; un sistema real recalcularía las formas\n",
    "                # para asegurar la compatibilidad o añadiría capas de adaptación.\n",
    "                # Por ahora, simplemente añadimos la conexión. La robustez del\n",
    "                # constructor del modelo (con MergeLayer) lo manejará.\n",
    "                print(f\"MUTATION: Adding skip connection from {source_gene['id']} to {dest_gene['id']}\")\n",
    "                add_connection_to_genome(new_genome, source_gene['id'], dest_gene['id'])\n",
    "\n",
    "    return new_genome\n",
    "\n",
    "def crossover_genomes(parent1, parent2):\n",
    "    \"\"\"\n",
    "    Realiza un cruce entre dos genomas padres.\n",
    "    Estrategia: Tomar la base convolucional de un padre y el clasificador del otro.\n",
    "    \"\"\"\n",
    "    child_genome = create_empty_genome()\n",
    "    \n",
    "    # Encontrar el punto de transición (flatten o global_pool) en el padre 1\n",
    "    p1_transition_idx = -1\n",
    "    for i, gene in enumerate(parent1['layers']):\n",
    "        if gene['type'] in ['flatten', 'global_pool']:\n",
    "            p1_transition_idx = i\n",
    "            break\n",
    "    if p1_transition_idx == -1: return copy.deepcopy(parent1) # No se pudo cruzar\n",
    "\n",
    "    # Copiar la parte convolucional del padre 1\n",
    "    for i in range(p1_transition_idx + 1):\n",
    "        add_layer_to_genome(child_genome, copy.deepcopy(parent1['layers'][i]))\n",
    "    for conn in parent1['connections']:\n",
    "        if conn[1] <= p1_transition_idx:\n",
    "            add_connection_to_genome(child_genome, conn[0], conn[1])\n",
    "\n",
    "    last_child_id = p1_transition_idx\n",
    "    \n",
    "    # Encontrar la forma de salida de la parte convolucional\n",
    "    # Para este ejemplo, asumiremos que se puede calcular o es conocida.\n",
    "    # Una implementación más avanzada recalcularía la forma exacta.\n",
    "    # Aquí lo simulamos buscando el primer lineal en el padre 2 para obtener las features.\n",
    "    p2_first_linear = next((g for g in parent2['layers'] if g['type'] == 'linear'), None)\n",
    "    if not p2_first_linear: return copy.deepcopy(parent1)\n",
    "\n",
    "    in_features = p2_first_linear['in_features']\n",
    "    # En un sistema real, se recalcularía la salida de la capa last_child_id\n",
    "    # y se ajustaría in_features de la primera capa lineal del hijo.\n",
    "    \n",
    "    # Copiar la parte lineal (clasificador) del padre 2\n",
    "    p2_linear_genes = [g for g in parent2['layers'] if g['type'] in ['linear', 'output']]\n",
    "    \n",
    "    id_map = {} # Mapeo de IDs antiguos (p2) a nuevos (hijo)\n",
    "    \n",
    "    # Conectar la parte conv del hijo con la parte lineal del padre 2\n",
    "    first_linear_gene = copy.deepcopy(p2_linear_genes[0])\n",
    "    first_linear_gene['in_features'] = in_features # Ajuste (simplificado)\n",
    "    new_id = add_layer_to_genome(child_genome, first_linear_gene)\n",
    "    id_map[p2_linear_genes[0]['id']] = new_id\n",
    "    add_connection_to_genome(child_genome, last_child_id, new_id)\n",
    "    last_child_id = new_id\n",
    "\n",
    "    for i in range(1, len(p2_linear_genes)):\n",
    "        gene = copy.deepcopy(p2_linear_genes[i])\n",
    "        old_id = gene['id']\n",
    "        \n",
    "        # Si es el nodo 'output', solo se ajusta su referencia 'from'\n",
    "        if gene['type'] == 'output':\n",
    "            gene['from'] = id_map[gene['from']]\n",
    "            add_layer_to_genome(child_genome, gene)\n",
    "            continue\n",
    "\n",
    "        new_id = add_layer_to_genome(child_genome, gene)\n",
    "        id_map[old_id] = new_id\n",
    "        \n",
    "        # Encontrar la conexión original en el padre 2 y recrearla con nuevos IDs\n",
    "        original_conn_source = next(s for s, d in parent2['connections'] if d == old_id)\n",
    "        add_connection_to_genome(child_genome, id_map[original_conn_source], new_id)\n",
    "\n",
    "    print(\"CROSSOVER: Created a new child genome.\")\n",
    "    return child_genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "42053f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== PADRE 1 ====================\n",
      "{0: (3, 32, 32), 1: (32, 16, 16), 2: (16, 8, 8), 3: (16,), 4: (512,)}\n",
      "--- GENOMA (Grafo) ---\n",
      "{'connections': [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5)],\n",
      " 'layers': [{'id': 0, 'type': 'input'},\n",
      "            {'activation': 'relu',\n",
      "             'id': 1,\n",
      "             'in_channels': 3,\n",
      "             'kernel': 3,\n",
      "             'out_channels': 32,\n",
      "             'output_shape': (32, 16, 16),\n",
      "             'padding': 1,\n",
      "             'pool': None,\n",
      "             'stride': 2,\n",
      "             'type': 'conv',\n",
      "             'use_bn': True},\n",
      "            {'activation': 'relu',\n",
      "             'id': 2,\n",
      "             'in_channels': 32,\n",
      "             'kernel': 3,\n",
      "             'out_channels': 16,\n",
      "             'output_shape': (16, 8, 8),\n",
      "             'padding': 1,\n",
      "             'pool': None,\n",
      "             'stride': 2,\n",
      "             'type': 'conv',\n",
      "             'use_bn': True},\n",
      "            {'id': 3, 'pool_type': 'adaptive_avg', 'type': 'global_pool'},\n",
      "            {'activation': 'gelu',\n",
      "             'dropout': 0.25,\n",
      "             'id': 4,\n",
      "             'in_features': 16,\n",
      "             'out_features': 512,\n",
      "             'type': 'linear'},\n",
      "            {'activation': 'none',\n",
      "             'dropout': 0.0,\n",
      "             'id': 5,\n",
      "             'in_features': 512,\n",
      "             'out_features': 10,\n",
      "             'type': 'linear'},\n",
      "            {'from': 5, 'id': 6, 'type': 'output'}]}\n",
      "\n",
      "--- ARQUITECTURA DEL MODELO ---\n",
      "GenomeNet(\n",
      "  (torch_layers): ModuleDict(\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "      (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Linear(in_features=16, out_features=512, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=10, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "Prueba de forward exitosa. Forma de salida: torch.Size([1, 10])\n",
      "\n",
      "==================== MUTACIÓN ====================\n",
      "--- GENOMA MUTADO ---\n",
      "{'connections': [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5)],\n",
      " 'layers': [{'id': 0, 'type': 'input'},\n",
      "            {'activation': 'relu',\n",
      "             'id': 1,\n",
      "             'in_channels': 3,\n",
      "             'kernel': 3,\n",
      "             'out_channels': 32,\n",
      "             'output_shape': (32, 16, 16),\n",
      "             'padding': 1,\n",
      "             'pool': None,\n",
      "             'stride': 2,\n",
      "             'type': 'conv',\n",
      "             'use_bn': True},\n",
      "            {'activation': 'relu',\n",
      "             'id': 2,\n",
      "             'in_channels': 32,\n",
      "             'kernel': 3,\n",
      "             'out_channels': 16,\n",
      "             'output_shape': (16, 8, 8),\n",
      "             'padding': 1,\n",
      "             'pool': None,\n",
      "             'stride': 2,\n",
      "             'type': 'conv',\n",
      "             'use_bn': True},\n",
      "            {'id': 3, 'pool_type': 'adaptive_avg', 'type': 'global_pool'},\n",
      "            {'activation': 'gelu',\n",
      "             'dropout': 0.0,\n",
      "             'id': 4,\n",
      "             'in_features': 16,\n",
      "             'out_features': 512,\n",
      "             'type': 'linear'},\n",
      "            {'activation': 'none',\n",
      "             'dropout': 0.0,\n",
      "             'id': 5,\n",
      "             'in_features': 512,\n",
      "             'out_features': 10,\n",
      "             'type': 'linear'},\n",
      "            {'from': 5, 'id': 6, 'type': 'output'}]}\n",
      "\n",
      "--- MODELO MUTADO ---\n",
      "GenomeNet(\n",
      "  (torch_layers): ModuleDict(\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "      (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Linear(in_features=16, out_features=512, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=10, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "==================== CRUCE ====================\n",
      "--- Generando Padre 2 ---\n",
      "{0: (3, 32, 32), 1: (128, 32, 32), 2: (128, 16, 16), 3: (16, 16, 16), 4: (16, 5, 5), 5: (400,), 6: (512,)}\n",
      "CROSSOVER: Created a new child genome.\n",
      "\n",
      "--- GENOMA HIJO ---\n",
      "{'connections': [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5)],\n",
      " 'layers': [{'id': 0, 'type': 'input'},\n",
      "            {'activation': 'relu',\n",
      "             'id': 1,\n",
      "             'in_channels': 3,\n",
      "             'kernel': 3,\n",
      "             'out_channels': 32,\n",
      "             'output_shape': (32, 16, 16),\n",
      "             'padding': 1,\n",
      "             'pool': None,\n",
      "             'stride': 2,\n",
      "             'type': 'conv',\n",
      "             'use_bn': True},\n",
      "            {'activation': 'relu',\n",
      "             'id': 2,\n",
      "             'in_channels': 32,\n",
      "             'kernel': 3,\n",
      "             'out_channels': 16,\n",
      "             'output_shape': (16, 8, 8),\n",
      "             'padding': 1,\n",
      "             'pool': None,\n",
      "             'stride': 2,\n",
      "             'type': 'conv',\n",
      "             'use_bn': True},\n",
      "            {'id': 3, 'pool_type': 'adaptive_avg', 'type': 'global_pool'},\n",
      "            {'activation': 'leaky_relu',\n",
      "             'dropout': 0.25,\n",
      "             'id': 4,\n",
      "             'in_features': 400,\n",
      "             'out_features': 512,\n",
      "             'type': 'linear'},\n",
      "            {'activation': 'none',\n",
      "             'dropout': 0.0,\n",
      "             'id': 5,\n",
      "             'in_features': 512,\n",
      "             'out_features': 10,\n",
      "             'type': 'linear'},\n",
      "            {'from': 5, 'id': 6, 'type': 'output'}]}\n",
      "\n",
      "--- MODELO HIJO ---\n",
      "GenomeNet(\n",
      "  (torch_layers): ModuleDict(\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "      (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Linear(in_features=400, out_features=512, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (2): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=10, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# ─── 6. EJECUCIÓN: DEMOSTRACIÓN COMPLETA ──────────────────────────────────────\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # (1) Generar un genoma aleatorio (Padre 1)\n",
    "    print(\"=\"*20 + \" PADRE 1 \" + \"=\"*20)\n",
    "    genome1 = random_genome_graph(input_shape=(3, 32, 32), num_classes=10)\n",
    "    print(\"--- GENOMA (Grafo) ---\")\n",
    "    pprint.pprint(genome1)\n",
    "    \n",
    "    # (2) Construir el modelo a partir del genoma\n",
    "    model1 = GenomeNet(genome1)\n",
    "    print(\"\\n--- ARQUITECTURA DEL MODELO ---\")\n",
    "    print(model1)\n",
    "    \n",
    "    # Probar con un tensor de entrada aleatorio\n",
    "    try:\n",
    "        dummy_input = torch.randn(1, 3, 32, 32)\n",
    "        output = model1(dummy_input)\n",
    "        print(f\"\\nPrueba de forward exitosa. Forma de salida: {output.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError durante la prueba de forward: {e}\")\n",
    "\n",
    "    # (3) Demostración de Mutación\n",
    "    print(\"\\n\" + \"=\"*20 + \" MUTACIÓN \" + \"=\"*20)\n",
    "    mutated_genome = mutate_genome(genome1)\n",
    "    print(\"--- GENOMA MUTADO ---\")\n",
    "    pprint.pprint(mutated_genome)\n",
    "    mutated_model = GenomeNet(mutated_genome)\n",
    "    print(\"\\n--- MODELO MUTADO ---\")\n",
    "    print(mutated_model)\n",
    "\n",
    "    # (4) Demostración de Cruce\n",
    "    print(\"\\n\" + \"=\"*20 + \" CRUCE \" + \"=\"*20)\n",
    "    # Generar un segundo padre\n",
    "    print(\"--- Generando Padre 2 ---\")\n",
    "    genome2 = random_genome_graph(input_shape=(3, 32, 32), num_classes=10)\n",
    "    # pprint.pprint(genome2)\n",
    "    \n",
    "    child_genome = crossover_genomes(genome1, genome2)\n",
    "    print(\"\\n--- GENOMA HIJO ---\")\n",
    "    pprint.pprint(child_genome)\n",
    "    child_model = GenomeNet(child_genome)\n",
    "    print(\"\\n--- MODELO HIJO ---\")\n",
    "    print(child_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "7c8e5977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: (3, 64, 64), 1: (64, 32, 32), 2: (32, 8, 8), 3: (32,), 4: (512,), 5: (512,)}\n",
      "GenomeNet(\n",
      "  (torch_layers): ModuleDict(\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "      (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Linear(in_features=32, out_features=512, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=10, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0109, -0.0070, -0.0120, -0.0877, -0.0277,  0.0397, -0.0256, -0.0249,\n",
       "          0.0071,  0.0037],\n",
       "        [ 0.0121, -0.0092, -0.0113, -0.0868, -0.0274,  0.0459, -0.0276, -0.0236,\n",
       "          0.0081,  0.0047],\n",
       "        [ 0.0116, -0.0120, -0.0141, -0.0894, -0.0318,  0.0473, -0.0256, -0.0240,\n",
       "          0.0093,  0.0048],\n",
       "        [ 0.0156, -0.0076, -0.0116, -0.0856, -0.0299,  0.0398, -0.0257, -0.0188,\n",
       "          0.0098,  0.0023],\n",
       "        [ 0.0153, -0.0094, -0.0154, -0.0860, -0.0291,  0.0425, -0.0233, -0.0192,\n",
       "          0.0091,  0.0061],\n",
       "        [ 0.0156, -0.0072, -0.0149, -0.0875, -0.0292,  0.0438, -0.0259, -0.0199,\n",
       "          0.0085,  0.0034],\n",
       "        [ 0.0142, -0.0095, -0.0115, -0.0888, -0.0287,  0.0411, -0.0280, -0.0212,\n",
       "          0.0094,  0.0062],\n",
       "        [ 0.0115, -0.0068, -0.0135, -0.0859, -0.0269,  0.0434, -0.0258, -0.0217,\n",
       "          0.0078,  0.0031],\n",
       "        [ 0.0141, -0.0106, -0.0138, -0.0877, -0.0289,  0.0406, -0.0260, -0.0199,\n",
       "          0.0066,  0.0040],\n",
       "        [ 0.0125, -0.0083, -0.0102, -0.0891, -0.0253,  0.0425, -0.0268, -0.0223,\n",
       "          0.0078,  0.0016],\n",
       "        [ 0.0118, -0.0082, -0.0125, -0.0883, -0.0262,  0.0417, -0.0292, -0.0232,\n",
       "          0.0085,  0.0024],\n",
       "        [ 0.0171, -0.0095, -0.0136, -0.0866, -0.0298,  0.0392, -0.0289, -0.0166,\n",
       "          0.0095,  0.0026]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genome1 = random_genome_graph(\n",
    "    input_shape=(3, 64, 64),\n",
    "    min_conv_layers=2, \n",
    "    max_conv_layers=5,\n",
    "    min_linear_layers=1,\n",
    "    max_linear_layers=2,\n",
    "    num_classes=10,\n",
    "    skip_connection_prob=0.0\n",
    ")\n",
    "model1 = GenomeNet(genome1)\n",
    "print(model1)\n",
    "\n",
    "x = torch.randn(12, 3, 64, 64)\n",
    "model1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "018148f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenomeNet(\n",
       "  (torch_layers): ModuleDict(\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(3, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv2d(32, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Conv2d(128, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "      (1): GELU(approximate='none')\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (1): GELU(approximate='none')\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (1): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (9): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "    )\n",
       "    (10): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (12): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=10, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "daecd66d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenomeNet(\n",
       "  (torch_layers): ModuleDict(\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): GELU(approximate='none')\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(32, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (1): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Linear(in_features=32, out_features=512, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=10, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open-nas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
